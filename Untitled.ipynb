{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20a9c3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin constructing GPTJModel...\n",
      "Embedding...\n",
      "Begin CellList...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "Begin constructing GPTJAttention...\n",
      "CellList complete\n",
      "Begin attention bias...\n",
      "Attention bias complete!\n",
      "GPTJModel constructed.\n"
     ]
    }
   ],
   "source": [
    "from model import GPTJForCausalLM, GPTJConfig\n",
    "model = GPTJForCausalLM(GPTJConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71bbdd94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CellGuard',\n",
       " 'IGNORE_LIST',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__del__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_add_attr',\n",
       " '_add_init_args',\n",
       " '_attr_synced',\n",
       " '_auto_parallel_compile_and_run',\n",
       " '_auto_parallel_mode',\n",
       " '_auto_prefix',\n",
       " '_backward_hook',\n",
       " '_bprop_debug',\n",
       " '_cast_mixed_precision_inputs',\n",
       " '_cell_tag',\n",
       " '_cells',\n",
       " '_check_construct_args',\n",
       " '_children_scope_recursive',\n",
       " '_construct_inputs_names',\n",
       " '_construct_inputs_num',\n",
       " '_create_time',\n",
       " '_del_attr',\n",
       " '_do_parameter_broadcast',\n",
       " '_get_construct_inputs_number_and_name',\n",
       " '_has_config_recompute',\n",
       " '_hook_construct',\n",
       " '_init_weights',\n",
       " '_load_inputs',\n",
       " '_mp_comm_recompute',\n",
       " '_parallel_inputs_run',\n",
       " '_parallel_optimizer_comm_recompute',\n",
       " '_parallel_parameter_merge_net_dict',\n",
       " '_parallel_parameter_name_list',\n",
       " '_param_prefix',\n",
       " '_parameter_layout_dict',\n",
       " '_params',\n",
       " '_params_list',\n",
       " '_phase',\n",
       " '_primitives',\n",
       " '_recompute',\n",
       " '_scope',\n",
       " '_set_attr_for_cell',\n",
       " '_set_attr_for_parameter',\n",
       " '_set_attr_for_parameter_tuple',\n",
       " '_set_gradient_checkpointing',\n",
       " '_set_recompute_scope',\n",
       " '_set_scope',\n",
       " '_sync_attr_for_compile',\n",
       " '_tensor_list',\n",
       " 'add_flags',\n",
       " 'add_flags_recursive',\n",
       " 'arguments_key',\n",
       " 'auto_parallel_compile_and_run',\n",
       " 'bprop_debug',\n",
       " 'cast',\n",
       " 'cast_inputs',\n",
       " 'cast_param',\n",
       " 'cell_init_args',\n",
       " 'cell_type',\n",
       " 'cells',\n",
       " 'cells_and_names',\n",
       " 'check_names',\n",
       " 'cls_name',\n",
       " 'compile',\n",
       " 'compile_and_run',\n",
       " 'construct',\n",
       " 'create_time',\n",
       " 'enable_hook',\n",
       " 'exec_checkpoint_graph',\n",
       " 'extend_repr',\n",
       " 'forward',\n",
       " 'generate_scope',\n",
       " 'get_flags',\n",
       " 'get_func_graph_proto',\n",
       " 'get_output_embeddings',\n",
       " 'get_parameters',\n",
       " 'get_scope',\n",
       " 'infer_param_pipeline_stage',\n",
       " 'init_parameters_data',\n",
       " 'insert_child_to_cell',\n",
       " 'insert_param_to_cell',\n",
       " 'load_parameter_slice',\n",
       " 'name_cells',\n",
       " 'parallel_parameter_merge_net_dict',\n",
       " 'parallel_parameter_name_list',\n",
       " 'param_prefix',\n",
       " 'parameter_broadcast_done',\n",
       " 'parameter_layout_dict',\n",
       " 'parameters_and_names',\n",
       " 'parameters_broadcast_dict',\n",
       " 'parameters_dict',\n",
       " 'phase',\n",
       " 'pipeline_stage',\n",
       " 'prepare_inputs_for_generation',\n",
       " 'pynative',\n",
       " 'recompute',\n",
       " 'register_backward_hook',\n",
       " 'remove_redundant_parameters',\n",
       " 'requires_grad',\n",
       " 'run_construct',\n",
       " 'set_auto_parallel',\n",
       " 'set_boost',\n",
       " 'set_broadcast_flag',\n",
       " 'set_comm_fusion',\n",
       " 'set_grad',\n",
       " 'set_output_embeddings',\n",
       " 'set_parallel_input_with_inputs',\n",
       " 'set_param_fl',\n",
       " 'set_param_ps',\n",
       " 'set_train',\n",
       " 'to_float',\n",
       " 'trainable_params',\n",
       " 'training',\n",
       " 'untrainable_params',\n",
       " 'update_cell_prefix',\n",
       " 'update_cell_type',\n",
       " 'update_parameters_name']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "103b9587",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Cell.parameters_dict of GPTJForCausalLM<\n",
       "  (transformer): GPTJModel<\n",
       "    (wte): Embedding<vocab_size=50400, embedding_size=4096, use_one_hot=False, embedding_table=Parameter (name=transformer.wte.embedding_table, shape=(50400, 4096), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
       "    (drop): Dropout<keep_prob=1.0>\n",
       "    (h): CellList<\n",
       "      (0): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.0.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.0.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (1): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.1.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.1.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (2): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.2.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.2.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (3): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.3.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.3.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (4): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.4.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.4.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (5): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.5.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.5.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (6): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.6.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.6.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (7): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.7.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.7.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (8): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.8.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.8.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (9): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.9.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.9.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (10): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.10.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.10.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (11): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.11.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.11.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (12): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.12.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.12.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (13): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.13.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.13.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (14): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.14.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.14.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (15): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.15.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.15.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (16): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.16.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.16.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (17): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.17.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.17.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (18): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.18.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.18.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (19): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.19.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.19.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (20): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.20.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.20.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (21): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.21.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.21.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (22): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.22.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.22.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (23): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.23.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.23.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (24): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.24.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.24.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (25): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.25.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.25.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (26): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.26.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.26.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (27): GPTJBlock<\n",
       "        (ln_1): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.27.ln_1.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.27.ln_1.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPTJAttention<\n",
       "          (attn_dropout): Dropout<keep_prob=1.0>\n",
       "          (resid_dropout): Dropout<keep_prob=1.0>\n",
       "          (k_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (v_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (q_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          (out_proj): Dense<input_channels=4096, output_channels=4096>\n",
       "          >\n",
       "        (mlp): GPTJMLP<\n",
       "          (fc_in): Dense<input_channels=4096, output_channels=16384, has_bias=True>\n",
       "          (fc_out): Dense<input_channels=16384, output_channels=4096, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      >\n",
       "    (ln_f): LayerNorm<normalized_shape=(4096,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.ln_f.gamma, shape=(4096,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.ln_f.beta, shape=(4096,), dtype=Float32, requires_grad=True)>\n",
       "    (tril): Tril<>\n",
       "    >\n",
       "  (lm_head): Dense<input_channels=4096, output_channels=50400, has_bias=True>\n",
       "  >>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af563706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
